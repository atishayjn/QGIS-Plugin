# -*- coding: utf-8 -*-
"""
/***************************************************************************
 RandomForestClassifier
                                 A QGIS plugin
 -
 Generated by Plugin Builder: http://g-sherman.github.io/Qgis-Plugin-Builder/
                              -------------------
        begin                : 2020-06-01
        git sha              : $Format:%H$
        copyright            : (C) 2020 by -
        email                : -
 ***************************************************************************/

/***************************************************************************
 *                                                                         *
 *   This program is free software; you can redistribute it and/or modify  *
 *   it under the terms of the GNU General Public License as published by  *
 *   the Free Software Foundation; either version 2 of the License, or     *
 *   (at your option) any later version.                                   *
 *                                                                         *
 ***************************************************************************/
"""
from qgis.PyQt.QtCore import QSettings, QTranslator, QCoreApplication
from qgis.PyQt.QtGui import QIcon
from qgis.PyQt.QtWidgets import QAction, QMessageBox
from qgis.core import QgsVectorLayer, QgsProject

# Initialize Qt resources from file resources.py
from .resources import *
# Import the code for the dialog
from .RandomForestClassifier_dialog import RandomForestClassifierDialog
from osgeo import gdal, gdal_array
import numpy as np
import pandas as pd
import os, os.path
import subprocess
import processing, sys
import glob


class RandomForestClassifier:
    """QGIS Plugin Implementation."""

    def __init__(self, iface):
        """Constructor.

        :param iface: An interface instance that will be passed to this class
            which provides the hook by which you can manipulate the QGIS
            application at run time.
        :type iface: QgsInterface
        """
        # Save reference to the QGIS interface
        self.iface = iface
        # initialize plugin directory
        self.plugin_dir = os.path.dirname(__file__)
        # initialize locale
        locale = QSettings().value('locale/userLocale')[0:2]
        locale_path = os.path.join(
            self.plugin_dir,
            'i18n',
            'RandomForestClassifier_{}.qm'.format(locale))

        if os.path.exists(locale_path):
            self.translator = QTranslator()
            self.translator.load(locale_path)
            QCoreApplication.installTranslator(self.translator)

        # Declare instance attributes
        self.actions = []
        self.menu = self.tr(u'&RandomForestClassifier')

        # Check if plugin was started the first time in current QGIS session
        # Must be set in initGui() to survive plugin reloads
        self.first_start = None

    # noinspection PyMethodMayBeStatic
    def tr(self, message):
        """Get the translation for a string using Qt translation API.

        We implement this ourselves since we do not inherit QObject.

        :param message: String for translation.
        :type message: str, QString

        :returns: Translated version of message.
        :rtype: QString
        """
        # noinspection PyTypeChecker,PyArgumentList,PyCallByClass
        return QCoreApplication.translate('RandomForestClassifier', message)

    def add_action(
            self,
            icon_path,
            text,
            callback,
            enabled_flag=True,
            add_to_menu=True,
            add_to_toolbar=True,
            status_tip=None,
            whats_this=None,
            parent=None):
        """Add a toolbar icon to the toolbar.

        :param icon_path: Path to the icon for this action. Can be a resource
            path (e.g. ':/plugins/foo/bar.png') or a normal file system path.
        :type icon_path: str

        :param text: Text that should be shown in menu items for this action.
        :type text: str

        :param callback: Function to be called when the action is triggered.
        :type callback: function

        :param enabled_flag: A flag indicating if the action should be enabled
            by default. Defaults to True.
        :type enabled_flag: bool

        :param add_to_menu: Flag indicating whether the action should also
            be added to the menu. Defaults to True.
        :type add_to_menu: bool

        :param add_to_toolbar: Flag indicating whether the action should also
            be added to the toolbar. Defaults to True.
        :type add_to_toolbar: bool

        :param status_tip: Optional text to show in a popup when mouse pointer
            hovers over the action.
        :type status_tip: str

        :param parent: Parent widget for the new action. Defaults None.
        :type parent: QWidget

        :param whats_this: Optional text to show in the status bar when the
            mouse pointer hovers over the action.

        :returns: The action that was created. Note that the action is also
            added to self.actions list.
        :rtype: QAction
        """

        icon = QIcon(icon_path)
        action = QAction(icon, text, parent)
        action.triggered.connect(callback)
        action.setEnabled(enabled_flag)

        if status_tip is not None:
            action.setStatusTip(status_tip)

        if whats_this is not None:
            action.setWhatsThis(whats_this)

        if add_to_toolbar:
            # Adds plugin icon to Plugins toolbar
            self.iface.addToolBarIcon(action)

        if add_to_menu:
            self.iface.addPluginToMenu(
                self.menu,
                action)

        self.actions.append(action)

        return action

    def initGui(self):
        """Create the menu entries and toolbar icons inside the QGIS GUI."""

        icon_path = ':/plugins/RandomForestClassifier/icon.png'
        self.add_action(
            icon_path,
            text=self.tr(u'RandomForestClassifier'),
            callback=self.run,
            parent=self.iface.mainWindow())

        # will be set False in run()
        self.first_start = True

    def unload(self):
        """Removes the plugin menu item and icon from QGIS GUI."""
        for action in self.actions:
            self.iface.removePluginMenu(
                self.tr(u'&RandomForestClassifier'),
                action)
            self.iface.removeToolBarIcon(action)

    # def show_pop(self):
    #     msg = QMessa

    # def saysome(self):
    #     print("Button Clicked")

    # ------------------------------------------------------------------------------------------------------------
    # def array2raster(newRasterfn, geotrans, proj, array):

    #     cols = array.shape[1]
    #     rows = array.shape[0]
    #     driver = gdal.GetDriverByName('GTiff')
    #     outRaster = driver.Create(newRasterfn, cols, rows, 1, gdal.GDT_Byte)
    #     outRaster.SetGeoTransform(geotrans)
    #     outband = outRaster.GetRasterBand(1)
    #     outband.WriteArray(array)
    #     outRaster.SetProjection(proj)
    #     outband.FlushCache()

    # ------------------------------------------------TOOLS-----------------------------------------------------------

    def vector2raster(self, v_path):
        # v_path = self.dlg.Train_img_label.filePath()
        print(v_path)

        vlayer = QgsVectorLayer(v_path, "Vector layer", "ogr")
        if not vlayer.isValid():
            print("Layer failed to load!")
        else:
            QgsProject.instance().addMapLayer(vlayer)
            print("Done")

        ext = vlayer.extent()
        # for feature in layer.getFeatures():
        #   ext = feature.geometry().boundingBox()  # this line is the relevant part
        xmin = ext.xMinimum()
        xmax = ext.xMaximum()
        ymin = ext.yMinimum()
        ymax = ext.yMaximum()
        print(xmin, ' ', xmax, ' ', ymax, ' ', ymin)

        extent_list = str(xmin) + ',' + str(xmax) + ',' + str(ymin) + ',' + str(ymax) + '[]'

        if not os.path.exists('QGIS_Temp'):
            os.makedirs('QGIS_Temp')

        r_path = 'QGIS_Temp/V2R_converted_temp.tif'

        processing.run("gdal:rasterize",
                       {'INPUT': v_path, 'FIELD': 'id', 'UNITS': 1, 'WIDTH': 0.001,
                        'HEIGHT': 0.001, 'EXTENT': extent_list, 'NODATA': 0,
                        'OPTIONS': '', 'DATA_TYPE': 5, 'INIT': None, 'INVERT': False, 'EXTRA': '',
                        'OUTPUT': r_path})

        return r_path

    def resampler(self, REF_IMG, IMG_LABEL):

        from osgeo import gdal, gdalconst

        self.dlg.train_progressBar.setValue(5)

        # IMG_ADD = self.dlg.train_img_add.filePath()
        # IMG_LABEL_ADD = self.dlg.train_img_label.filePath()

        input1 = gdal.Open(IMG_LABEL, gdalconst.GA_ReadOnly)
        inputProj = input1.GetProjection()
        inputTrans = input1.GetGeoTransform()

        reference = gdal.Open(REF_IMG, gdalconst.GA_ReadOnly)
        referenceProj = reference.GetProjection()
        referenceTrans = reference.GetGeoTransform()
        bandreference = reference.GetRasterBand(1)
        x = reference.RasterXSize
        y = reference.RasterYSize

        RESAMPLED_IMG_LABEL = "Delhi_ROI_resampled2.tif"  # Path to output file
        driver = gdal.GetDriverByName('GTiff')
        output = driver.Create(RESAMPLED_IMG_LABEL, x, y, 1, bandreference.DataType)
        output.SetGeoTransform(referenceTrans)
        output.SetProjection(referenceProj)

        gdal.ReprojectImage(input1, output, inputProj, referenceProj, gdalconst.GRA_Bilinear)

        del output

        return RESAMPLED_IMG_LABEL

    # -------------------------------------------TILES GENERATOR-------------------------------------------------
    def tiles(self):

        in_path = self.dlg.Tiles_Input.filePath()
        # in_path = os.path.normpath(in_path)
        # print(type(in_path))
        # print(in_path)

        out_path = self.dlg.Tiles_Output.filePath()

        if (not in_path):
            print("Enter Input")
            QMessageBox.critical(self.dlg, 'Invalid Input', 'Enter the address of the image to be splitted.')
            return

        if (not out_path):
            print("Enter Input")
            QMessageBox.critical(self.dlg, 'Invalid Input', 'Enter the address of Output Location to store tiles.')
            return

        file_name = 'Tile'

        img_out_path = os.path.join(out_path, "Tiles")

        if not os.path.exists(img_out_path):
            os.makedirs(img_out_path)

        # GIVE OPTION TO OVERWRITE HERE AND WARN

        files_out_path = os.path.join(img_out_path, file_name)

        tile_size_x = self.dlg.TileSizeX.value()
        tile_size_y = self.dlg.TileSizeY.value()

        if (not tile_size_x):
            tile_size_x = int(self.dlg.TileSizeX.defaultValue())
        else:
            tile_size_x = int(tile_size_x)

        if (not tile_size_y):
            tile_size_y = int(self.dlg.TileSizeX.defaultValue())
        else:
            tile_size_y = int(tile_size_y)

        ds = gdal.Open(in_path)

        complete = 0
        self.dlg.Tile_progressBar.setValue(complete)

        # CREATE_NO_WINDOW = 0x08000000

        # for i in range(5):
        band = ds.GetRasterBand(1)
        xsize = band.XSize
        ysize = band.YSize
        print(band)
        print(xsize)
        print(ysize)

        inc = (tile_size_x * tile_size_y) / (xsize * ysize) * 100

        for i in range(0, xsize, tile_size_x):
            for j in range(0, ysize, tile_size_y):
                com_string = "gdal_translate -of GTIFF -srcwin " + str(i) + ", " + str(j) + ", " + str(
                    tile_size_x) + ", " + str(tile_size_y) + " " + str(in_path) + " " + str(files_out_path) + str(
                    i) + "_" + str(j) + ".tif"
                subprocess.call(com_string, creationflags=subprocess.CREATE_NO_WINDOW)
                complete = complete + inc
                self.dlg.Tile_progressBar.setValue(complete)

        self.dlg.Tile_progressBar.setValue(100)

        label_path_v = self.dlg.Tiles_Input_2.filePath()

        if (not label_path_v):
            return

        revstr = label_path_v[::-1]
        splitrevstr = revstr.split('.')
        if splitrevstr[0] == 'ffit':
            label_path = self.dlg.Tiles_Input_2.filePath()
        else:
            label_path_r = self.vector2raster(label_path_v)
            label_path = self.resampler(in_path, label_path_r)

        label_out_path = os.path.join(out_path, "Labels")
        file_name = 'Tile'

        if not os.path.exists(label_out_path):
            os.makedirs(label_out_path)

        # GIVE OPTION TO OVERWRITE HERE AND WARN

        file_out_path = os.path.join(label_out_path, file_name)

        tile_size_x = self.dlg.TileSizeX.value()
        tile_size_y = self.dlg.TileSizeY.value()

        if (not tile_size_x):
            tile_size_x = int(self.dlg.TileSizeX.defaultValue())
        else:
            tile_size_x = int(tile_size_x)

        if (not tile_size_y):
            tile_size_y = int(self.dlg.TileSizeX.defaultValue())
        else:
            tile_size_y = int(tile_size_y)

        ds = gdal.Open(label_path)

        complete = 0
        self.dlg.Tile_progressBar.setValue(complete)

        # CREATE_NO_WINDOW = 0x08000000

        # for i in range(5):
        band = ds.GetRasterBand(1)
        xsize = band.XSize
        ysize = band.YSize

        inc = (tile_size_x * tile_size_y) / (xsize * ysize) * 100

        for i in range(0, xsize, tile_size_x):
            for j in range(0, ysize, tile_size_y):
                com_string = "gdal_translate -of GTIFF -srcwin " + str(i) + ", " + str(j) + ", " + str(
                    tile_size_x) + ", " + str(tile_size_y) + " " + str(label_path) + " " + str(file_out_path) + str(
                    i) + "_" + str(j) + ".tif"
                subprocess.call(com_string, creationflags=subprocess.CREATE_NO_WINDOW)
                complete = complete + inc
                self.dlg.Tile_progressBar.setValue(complete)
        self.dlg.Tile_progressBar.setValue(100)

    # -------------------------------------------CLASSIFIERS---------------------------------------------------------

    def svm(self):

        import pickle, subprocess

        try:
            from sklearn.model_selection import train_test_split
            from sklearn.svm import SVC
            from sklearn.metrics import confusion_matrix, classification_report
        except ImportError:
            print("scikit-learn package not present\nInstalling...")
            # import pip
            # pip.main(["install", "--user", "scikit-learn"])
            subprocess.call("pip install --user scikit-learn", creationflags=subprocess.CREATE_NEW_CONSOLE)

            from sklearn.model_selection import train_test_split
            from sklearn.svm import SVC
            from sklearn.metrics import confusion_matrix, classification_report

        self.dlg.Clfr_progressBar.setValue(30)

        IMAGE_ADD = self.dlg.input_img_box.filePath()
        MODEL_ADD = self.dlg.input_img_box_2.filePath()

        # #To open the image:
        img_ds = gdal.Open(IMAGE_ADD, gdal.GA_ReadOnly)

        img = np.zeros((img_ds.RasterYSize, img_ds.RasterXSize, img_ds.RasterCount),
                       gdal_array.GDALTypeCodeToNumericTypeCode(img_ds.GetRasterBand(1).DataType))

        for b in range(img.shape[2]):
            img[:, :, b] = img_ds.GetRasterBand(b + 1).ReadAsArray()

        print(img.shape)

        with open(MODEL_ADD, 'rb') as f:
            svm = pickle.load(f)

            img_as_array = img.reshape(-1, img.shape[2])
            print('Reshaped from {n} to {o}'.format(o=img.shape,
                                                    n=img_as_array.shape))

        self.dlg.Clfr_progressBar.setValue(60)

        class_prediction = svm.predict(img_as_array)

        class_prediction = class_prediction.reshape(img[:, :, 0].shape)
        print(class_prediction.shape)

        geotrans = img_ds.GetGeoTransform()
        proj = img_ds.GetProjection()

        fname = 'SVM_classified.tif'

        # fname = self.dlg.input_img_box_3.filePath()

        # To convert array to raster
        cols = class_prediction.shape[1]
        rows = class_prediction.shape[0]
        driver = gdal.GetDriverByName('GTiff')
        outRaster = driver.Create(fname, cols, rows, 1, gdal.GDT_Byte)
        outRaster.SetGeoTransform(geotrans)
        outRaster.SetProjection(proj)
        outband = outRaster.GetRasterBand(1)
        outband.WriteArray(class_prediction)
        outband.FlushCache()

        self.dlg.Clfr_progressBar.setValue(100)

    def UNET_Classifier(self):

        # Store and check inputs

        THRESHOLD = 0.5

        TILES_ADD = self.dlg.input_img_box.filePath()
        MODEL_ADD = self.dlg.input_img_box_2.filePath()
        H5_ADD = self.dlg.input_img_box_6.filePath()
        OUT_ADD = self.dlg.classifier_output.filePath()

        tiles_merge = []

        # import statements

        try:
            import tensorflow as tf
            from keras.models import Model, load_model, model_from_json
            from keras.preprocessing.image import img_to_array, load_img

        except ImportError:
            print("tensorflow not present\nInstalling...")
            # import pip
            # pip.main(["install", "--user", "tensorflow"])

            subprocess.call("pip install --user tensorflow", creationflags=subprocess.CREATE_NEW_CONSOLE)

            import tensorflow as tf
            from keras.models import Model, load_model, model_from_json
            from keras.preprocessing.image import img_to_array, load_img

        try:
            from skimage.transform import resize

        except ImportError:
            print("scikit-image package not present\nInstalling...")
            # import pip
            # pip.main(["install", "--user", "scikit-image"])

            subprocess.call("pip install --user scikit-image", creationflags=subprocess.CREATE_NEW_CONSOLE)

            from skimage.transform import resize

        self.dlg.Clfr_progressBar.setValue(30)

        # Load Tiles
        for tile in os.listdir(TILES_ADD):
            if tile.endswith(".tif"):
                IMG_ADD = os.path.join(TILES_ADD, tile)

                print('Processing: ', IMG_ADD)

                img_ds = gdal.Open(IMG_ADD, gdal.GA_ReadOnly)

                img = np.zeros((img_ds.RasterYSize, img_ds.RasterXSize, img_ds.RasterCount),
                               gdal_array.GDALTypeCodeToNumericTypeCode(img_ds.GetRasterBand(1).DataType))

                for b in range(img.shape[2]):
                    img[:, :, b] = img_ds.GetRasterBand(b + 1).ReadAsArray()

                x_img = resize(img, (512, 512), mode='constant', preserve_range=True)

                print(x_img)

                self.dlg.Clfr_progressBar.setValue(50)

                with open(MODEL_ADD, 'r') as json_file:
                    loaded_nnet = json_file.read()

                save_model = tf.keras.models.model_from_json(loaded_nnet)
                save_model.load_weights(H5_ADD)

                self.dlg.Clfr_progressBar.setValue(70)

                pred = save_model.predict(np.expand_dims(x_img, axis=0), verbose=1)[0, :, :,
                       0]  # [ : : ] to squeeze the image dimension equiv to pred[0].squeeze()

                pred_bin = np.where(pred > THRESHOLD, 1, 0)

                self.dlg.Clfr_progressBar.setValue(90)

                print(pred_bin.shape)

                geotrans = img_ds.GetGeoTransform()
                proj = img_ds.GetProjection()

                temp_dir = 'QAi_Temp_Tiles'

                if not os.path.exists(temp_dir):
                    os.makedirs(temp_dir)

                fpath = os.path.join(temp_dir, str(tile))

                cols = img.shape[1]
                rows = img.shape[0]
                pred_bin = resize(pred_bin, (rows, cols), mode='constant', preserve_range=True)
                driver = gdal.GetDriverByName('GTiff')
                outRaster = driver.Create(fpath, cols, rows, 1, gdal.GDT_Byte)
                outRaster.SetGeoTransform(geotrans)
                outRaster.SetProjection(proj)
                outband = outRaster.GetRasterBand(1)
                outband.WriteArray(pred_bin)
                outband.FlushCache()

                tiles_merge.append(fpath)

        print('Image Saved.')

        # MERGE TILES

        save_file = 'Classified_Merged_Image.tif'

        out_img = os.path.join(OUT_ADD, save_file)

        processing.run("gdal:merge", {'INPUT': tiles_merge, 'PCT': 'False',
                                      'SEPERATE': 'False', 'DATA_TYPE': 1, 'NODATA_INPUT': None, 'NODATA_OUTPUT': None,
                                      'OPTIONS': 'High Compression', 'EXTRA': 'None',
                                      'OUTPUT': out_img})
        print("Tiles Merged!!")

        self.dlg.Clfr_progressBar.setValue(100)

    def SatNet_Classifier(self):

        import matplotlib.pyplot as plt

        try:
            import tensorflow as tf
            from keras.models import Model, load_model, model_from_json
            from keras.preprocessing.image import img_to_array, load_img

        except ImportError:
            print("tensorflow not present\nInstalling...")
            # import pip
            # pip.main(["install", "--user", "tensorflow"])

            subprocess.call("pip install --user tensorflow", creationflags=subprocess.CREATE_NEW_CONSOLE)

            import tensorflow as tf
            from keras.models import Model, load_model, model_from_json
            from keras.preprocessing.image import img_to_array, load_img

        try:
            from skimage.transform import resize

        except ImportError:
            print("scikit-image package not present\nInstalling...")
            # import pip
            # pip.main(["install", "--user", "scikit-image"])

            subprocess.call("pip install --user scikit-image", creationflags=subprocess.CREATE_NEW_CONSOLE)

            from skimage.transform import resize

        self.dlg.Clfr_progressBar.setValue(30)

        THRESHOLD = 0.5

        # TILES_ADD = "C:\\Users\\Atishay\\Desktop\\Img/"
        TILES_ADD = self.dlg.input_img_box.filePath()
        MODEL_ADD = self.dlg.input_img_box_2.filePath()
        H5_ADD = self.dlg.input_img_box_6.filePath()

        with open(MODEL_ADD, 'r') as json_file:
            loaded_nnet = json_file.read()

        save_model = tf.keras.models.model_from_json(loaded_nnet)
        save_model.load_weights(H5_ADD)

        for tile in os.listdir(TILES_ADD):
            if tile.endswith(".tif"):
                IMG_ADD = os.path.join(TILES_ADD, tile)

                print('Processing: ', IMG_ADD)

                # To open the image:
                img_ds = gdal.Open(IMG_ADD, gdal.GA_ReadOnly)

                img = np.zeros((img_ds.RasterYSize, img_ds.RasterXSize, img_ds.RasterCount),
                               gdal_array.GDALTypeCodeToNumericTypeCode(img_ds.GetRasterBand(1).DataType))

                for b in range(img.shape[2]):
                    img[:, :, b] = img_ds.GetRasterBand(b + 1).ReadAsArray()

                x_img = img

                # ----------------------------TAKE NOTE HERE ON INPUT SIZE-----------------------
                # x_img = resize(x_img, (64, 64), mode='constant', preserve_range=True)
                # x_img.shape

                self.dlg.Clfr_progressBar.setValue(50)

                self.dlg.Clfr_progressBar.setValue(70)

                # ------------------TAKE NOTE HERE ON OUTPUT DIM-----------------

                # pred = save_model.predict(np.expand_dims(x_img, axis=0), verbose=1)[0, :, :, 0]  # [ : : ] to squeeze the image dimension equiv to pred[0].squeeze()

                x_img1 = np.expand_dims(x_img, axis=0)
                pred_bin = save_model.predict(x_img1, verbose=1)
                img_class = pred_bin.argmax() + 1

                self.dlg.Clfr_progressBar.setValue(90)

                print(img_class)

                out_img = np.zeros((img_ds.RasterYSize, img_ds.RasterXSize))
                out_img[:, :] = 255 / img_class

                geotrans = img_ds.GetGeoTransform()
                proj = img_ds.GetProjection()

                if not os.path.exists('Tiles'):
                    os.makedirs('Tiles')
                fname = 'Tiles/' + str(tile)

                cols = out_img.shape[1]
                rows = out_img.shape[0]
                driver = gdal.GetDriverByName('GTiff')
                outRaster = driver.Create(fname, cols, rows, 1, gdal.GDT_Byte)
                outRaster.SetGeoTransform(geotrans)
                outRaster.SetProjection(proj)
                outband = outRaster.GetRasterBand(1)
                outband.WriteArray(out_img)
                outband.FlushCache()

        self.dlg.Clfr_progressBar.setValue(100)

    def randomForest(self):

        # Store and Check Input---------------------------------------

        IMAGE_ADD = self.dlg.input_img_box.filePath()
        MODEL_ADD = self.dlg.input_img_box_2.filePath()
        OUT_ADD = self.dlg.classifier_output.filePath()

        if (not IMAGE_ADD):
            QMessageBox.critical(self.dlg, 'Invalid Input', 'Enter the address of Image to be classified.')
            return

        if (not MODEL_ADD):
            QMessageBox.critical(self.dlg, 'Invalid Input', 'Enter the address of saved model.')
            return

        if (not OUT_ADD):
            QMessageBox.critical(self.dlg, 'Invalid Input', 'Enter the output address.')
            return

        # import statements

        from osgeo import gdal, gdal_array
        import numpy as np
        import pickle, subprocess

        try:
            from sklearn.model_selection import train_test_split
            from sklearn.ensemble import RandomForestClassifier
            from sklearn.metrics import confusion_matrix, classification_report
        except ImportError:
            print("scikit-learn package not present\nInstalling...")
            # import pip
            # pip.main(["install", "--user", "scikit-learn"])
            subprocess.call("pip install --user scikit-learn", creationflags=subprocess.CREATE_NEW_CONSOLE)

            from sklearn.model_selection import train_test_split
            from sklearn.ensemble import RandomForestClassifier
            from sklearn.metrics import confusion_matrix, classification_report

        self.dlg.Clfr_progressBar.setValue(30)

        # OUTPUT_ADD = fp3

        # #To open the image:
        img_ds = gdal.Open(IMAGE_ADD, gdal.GA_ReadOnly)

        img = np.zeros((img_ds.RasterYSize, img_ds.RasterXSize, img_ds.RasterCount),
                       gdal_array.GDALTypeCodeToNumericTypeCode(img_ds.GetRasterBand(1).DataType))

        for b in range(img.shape[2]):
            img[:, :, b] = img_ds.GetRasterBand(b + 1).ReadAsArray()

        print('Dimensions of Input Image: ', img.shape)

        with open(MODEL_ADD, 'rb') as f:
            model = pickle.load(f)

            img_as_array = img.reshape(-1, img.shape[2])
            print('Reshaped from {n} to {o}'.format(o=img.shape,
                                                    n=img_as_array.shape))

        self.dlg.Clfr_progressBar.setValue(60)

        class_prediction = model.predict(img_as_array)

        class_prediction = class_prediction.reshape(img[:, :, 0].shape)
        print(class_prediction.shape)

        # fname = self.dlg.input_img_box_3.filePath()

        # self.array2raster(fname, geotrans, proj, class_prediction)

        # To convert array to raster

        geotrans = img_ds.GetGeoTransform()
        proj = img_ds.GetProjection()
        fname = 'Image_classified.tif'

        fpath = os.path.join(OUT_ADD, fname)

        cols = class_prediction.shape[1]
        rows = class_prediction.shape[0]
        driver = gdal.GetDriverByName('GTiff')
        outRaster = driver.Create(fpath, cols, rows, 1, gdal.GDT_Byte)
        outRaster.SetGeoTransform(geotrans)
        outRaster.SetProjection(proj)
        outband = outRaster.GetRasterBand(1)
        outband.WriteArray(class_prediction)
        outband.FlushCache()

        self.dlg.Clfr_progressBar.setValue(100)

    # -------------------------------------------TRAIN---------------------------------------------------------
    def rfc_train(self):

        # Store and Check Inputs---------------------------------------------------------------
        IMG_ADD = self.dlg.train_img_add.filePath()
        IMG_VLABEL_ADD = self.dlg.train_img_label.filePath()
        OUT_ADD = self.dlg.train_output.filePath()

        if (not IMG_ADD):
            print("Enter Input")
            QMessageBox.critical(self.dlg, 'Invalid Input', 'Enter the address of Image to be classified.')
            return
        if (not IMG_VLABEL_ADD):
            print("Enter Input")
            QMessageBox.critical(self.dlg, 'Invalid Input', 'Enter the address of Image Label (Annotation).')
            return
        if (not OUT_ADD):
            print("Enter Input")
            QMessageBox.critical(self.dlg, 'Invalid Input', 'Enter the address to save model.')
            return

        VALIDATION_SPLIT = self.dlg.train_valRatio.currentText()  # TO BE TAKEN FROM USER (Train Val Ratio)

        if (not VALIDATION_SPLIT):
            VALIDATION_SPLIT = 0.2
        else:
            VALIDATION_SPLIT = float(VALIDATION_SPLIT)

        num_trees = self.dlg.train_RFC_trees.value()
        if (not num_trees):
            num_trees = int(self.dlg.train_RFC_trees.defaultValue())
        else:
            try:
                num_trees = abs(int(num_trees))
            except ValueError:
                QMessageBox.critical(self.dlg, 'Invalid Input', 'Enter a positive number for Number of Trees.')
                return

        Depth = self.dlg.train_RFC_Depth.value()
        if (not Depth):
            Depth = None
        else:
            try:
                Depth = abs(int(Depth))
            except ValueError:
                QMessageBox.critical(self.dlg, 'Invalid Input',
                                     'Enter a positive number for Depth or leave blank for maximum depth.')
                return

                # import libraries-----------------------------------------------------------------
        from osgeo import gdal, gdal_array
        import numpy as np
        import pickle

        try:
            from sklearn.model_selection import train_test_split
            from sklearn.ensemble import RandomForestClassifier
            from sklearn.metrics import confusion_matrix, classification_report
        except ImportError:
            print("scikit-learn package not present\nInstalling...")
            # import pip
            # pip.main(["install", "--user", "scikit-learn"])
            subprocess.call("pip install --user scikit-learn", creationflags=subprocess.CREATE_NEW_CONSOLE)

            from sklearn.model_selection import train_test_split
            from sklearn.ensemble import RandomForestClassifier
            from sklearn.metrics import confusion_matrix, classification_report

        self.dlg.train_progressBar.setValue(20)  # For progress bar

        # ADD AN OPTION HERE TO CHECK IF THE LABEL FILE IS ALREADY RASTER

        IMG_LABEL_ADD = self.vector2raster(IMG_VLABEL_ADD)
        RESAMPLED_IMG_LABEL1 = self.resampler(IMG_ADD, IMG_LABEL_ADD)

        self.dlg.train_progressBar.setValue(40)

        # LOAD IMAGES
        img_ds = gdal.Open(IMG_ADD, gdal.GA_ReadOnly)

        img = np.zeros((img_ds.RasterYSize, img_ds.RasterXSize, img_ds.RasterCount),
                       gdal_array.GDALTypeCodeToNumericTypeCode(img_ds.GetRasterBand(1).DataType))

        for b in range(img.shape[2]):
            img[:, :, b] = img_ds.GetRasterBand(b + 1).ReadAsArray()

        print('Size of Input Image: ', img.shape)

        roi_ds = gdal.Open(RESAMPLED_IMG_LABEL1, gdal.GA_ReadOnly)

        roi = roi_ds.GetRasterBand(1).ReadAsArray().astype(np.uint8)

        print('Size of Annotation Image: ', roi.shape)

        self.dlg.train_progressBar.setValue(50)

        np.vstack(np.unique(roi, return_counts=True)).T

        # Extracting Annotated region for training
        features = img[roi > 0, :]
        labels = roi[roi > 0]
        print(features.shape)
        print(labels.shape)

        X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=VALIDATION_SPLIT,
                                                            random_state=0)

        print('Dimensions of Training Data: ', X_train.shape)
        print('Dimensions of Testing Data: ', X_test.shape)

        self.dlg.train_progressBar.setValue(60)

        rf = RandomForestClassifier(n_estimators=num_trees, max_depth=None, n_jobs=-1,
                                    oob_score=True)  # n_estim = Trees, max_depth = Depth

        rf.fit(X_train, y_train)

        # Classification Report
        class_predict = rf.predict(X_test)
        class_report = classification_report(y_test, class_predict, output_dict=True)
        df = pd.DataFrame(class_report).transpose()
        report_file = "Classification_Report_RFC.csv"
        report_path = os.path.join(OUT_ADD, report_file)
        
        try:
            df.to_csv(report_path)
        except PermissionError:
            QMessageBox.critical(self.dlg, 'Permission Denied',
                                 'Unable to save the file in the specified location. Please choose a different address to save the file.')
            return

        self.dlg.train_progressBar.setValue(80)

        # Save Model
        model_file = 'RFC_model_trained.sav'
        file_path = os.path.join(OUT_ADD, model_file)

        try:
            pickle.dump(rf, open(file_path, 'wb'))
        except PermissionError:
            QMessageBox.critical(self.dlg, 'Permission Denied',
                                 'Unable to save the file in the specified location. Please choose a different address to save the file.')
            return

        self.dlg.train_progressBar.setValue(100)

    def svm_train(self):  #To-do: Check input and update wrt RFC

        from osgeo import gdal, gdal_array
        import numpy as np
        import pickle

        try:
            from sklearn.model_selection import train_test_split
            from sklearn.svm import SVC
            from sklearn.metrics import confusion_matrix, classification_report
        except ImportError:
            print("scikit-learn package not present\nInstalling...")
            # import pip
            # pip.main(["install", "--user", "scikit-learn"])
            subprocess.call("pip install --user scikit-learn", creationflags=subprocess.CREATE_NEW_CONSOLE)

            from sklearn.model_selection import train_test_split
            from sklearn.svm import SVC
            from sklearn.metrics import confusion_matrix, classification_report

        self.dlg.train_progressBar.setValue(20)

        IMG_ADD = self.dlg.train_img_add.filePath()
        IMG_VLABEL_ADD = self.dlg.train_img_label.filePath()

        IMG_LABEL_ADD = self.vector2raster(IMG_VLABEL_ADD)
        RESAMPLED_IMG_LABEL1 = self.resampler(IMG_ADD, IMG_LABEL_ADD)

        OUT_ADD = self.dlg.train_output.filePath()

        VALIDATION_SPLIT = self.dlg.train_valRatio.currentText()  # TO BE TAKEN FROM USER (Train Val Ratio)

        if (not VALIDATION_SPLIT):
            VALIDATION_SPLIT = 0.2
        else:
            VALIDATION_SPLIT = float(VALIDATION_SPLIT)

        if (not IMG_ADD):
            print("Enter Input")
            QMessageBox.critical(self.dlg, 'No Input', 'Please select the image to be Trained.')
            return
        if (not IMG_LABEL_ADD):
            print("Enter Input")
            QMessageBox.critical(self.dlg, 'No Input', 'Please input the Label.')
            return

        self.dlg.train_progressBar.setValue(40)

        img_ds = gdal.Open(IMG_ADD, gdal.GA_ReadOnly)

        img = np.zeros((img_ds.RasterYSize, img_ds.RasterXSize, img_ds.RasterCount),
                       gdal_array.GDALTypeCodeToNumericTypeCode(img_ds.GetRasterBand(1).DataType))

        for b in range(img.shape[2]):
            img[:, :, b] = img_ds.GetRasterBand(b + 1).ReadAsArray()

        print(img.shape)

        roi_ds = gdal.Open(RESAMPLED_IMG_LABEL1, gdal.GA_ReadOnly)

        roi = roi_ds.GetRasterBand(1).ReadAsArray().astype(np.uint8)

        print(roi.shape)

        self.dlg.train_progressBar.setValue(50)

        np.vstack(np.unique(roi, return_counts=True)).T

        features = img[roi > 0, :]
        labels = roi[roi > 0]
        print(features.shape)
        print(labels.shape)

        X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=VALIDATION_SPLIT,
                                                            random_state=0)

        print(X_train.shape)
        print(X_test.shape)

        self.dlg.train_progressBar.setValue(60)

        svm = SVC()

        svm.fit(X_train, y_train)
        
        # Classification Report
        class_predict = svm.predict(X_test)
        class_report = classification_report(y_test, class_predict, output_dict=True)
        df = pd.DataFrame(class_report).transpose()
        report_file = "Classification_Report_SVM.csv"
        report_path = os.path.join(OUT_ADD, report_file)
        
        try:
            df.to_csv(report_path)
        except PermissionError:
            QMessageBox.critical(self.dlg, 'Permission Denied',
                                 'Unable to save the file in the specified location. Please choose a different address to save the file.')
            return

        self.dlg.train_progressBar.setValue(80)

        # Save Model
        model_file = 'svm_model_trained.sav'
        file_path = os.path.join(OUT_ADD, model_file)

        try:
            pickle.dump(svm, open(file_path, 'wb'))
        except PermissionError:
            QMessageBox.critical(self.dlg, 'Permission Denied',
                                 'Unable to save the file in the specified location. Please choose a different address to save the file.')
            return

        self.dlg.train_progressBar.setValue(100)

    def UNET_build(self):

        # imports-----------------------------------------------------------
        try:
            import tensorflow as tf
            import tensorflow.keras.backend as K
            from tensorflow.keras.layers import (Dropout,
                                                 Conv2D,
                                                 MaxPooling2D,
                                                 Conv2DTranspose,
                                                 concatenate,
                                                 BatchNormalization,
                                                 Activation,
                                                 Conv2DTranspose,
                                                 Add)
            from tensorflow.keras import Input
            from tensorflow.keras.callbacks import (EarlyStopping,
                                                    ModelCheckpoint,
                                                    ReduceLROnPlateau)
            from tensorflow.keras.preprocessing.image import (ImageDataGenerator,
                                                              array_to_img,
                                                              img_to_array,
                                                              load_img)

        except ImportError:
            print("tensorflow not present\nInstalling...")
            # import pip
            # pip.main(["install", "--user", "tensorflow"])

            subprocess.call("pip install --user tensorflow", creationflags=subprocess.CREATE_NEW_CONSOLE)

            import tensorflow as tf
            import tensorflow.keras.backend as K
            from tensorflow.keras.layers import (Dropout,
                                                 Conv2D,
                                                 MaxPooling2D,
                                                 Conv2DTranspose,
                                                 concatenate,
                                                 BatchNormalization,
                                                 Activation,
                                                 Conv2DTranspose,
                                                 Add)
            from tensorflow.keras import Input
            from tensorflow.keras.callbacks import (EarlyStopping,
                                                    ModelCheckpoint,
                                                    ReduceLROnPlateau)
            from tensorflow.keras.preprocessing.image import (ImageDataGenerator,
                                                              array_to_img,
                                                              img_to_array,
                                                              load_img)
        try:
            from skimage.transform import resize

        except ImportError:
            print("scikit-image package not present\nInstalling...")
            # import pip
            # pip.main(["install", "--user", "scikit-image"])

            subprocess.call("pip install --user scikit-image", creationflags=subprocess.CREATE_NEW_CONSOLE)

            from skimage.transform import resize

            # Output Location

        OUT_ADD = self.dlg.Build_out.filePath()

        # ACCESORY FUNCTION:-------------------------------

        f = 0

        def input_check(depth, arg_list):
            length = len(arg_list)
            if (length == depth or length == 1):
                return 0
            else:
                return 1

        # Model Parameters[From Input Fields]--------------------------------------------

        # Number of bands in input image
        # N_CHANNELS = 3

        N_CHANNELS = self.dlg.Build_bands.value()
        if (not N_CHANNELS):
            N_CHANNELS = int(self.dlg.Build_bands.defaultValue())
        else:
            N_CHANNELS = int(N_CHANNELS)

        # Number of classes to be classified
        # N_CLASSES = 1

        N_CLASSES = self.dlg.Build_classes.value()
        if (not N_CLASSES):
            N_CLASSES = int(self.dlg.Build_classes.defaultValue())
        else:
            N_CLASSES = int(N_CLASSES)

        # Depth of the model
        # M_DEPTH = 5

        M_DEPTH = self.dlg.Build_depth.value()
        if (not M_DEPTH):
            M_DEPTH = int(self.dlg.Build_depth.defaultValue())
        else:
            M_DEPTH = int(M_DEPTH)

        # Dropout rate
        # Either specify only 1 value common across whole model
        # OR
        # Specify exactly M_DEPTH vales, these will be reflected to for decoder
        # M_DROPOUT_RATE = [0.25]
        M_DROPOUT_RATE = list()
        dor_inp = self.dlg.Build_dout.value()
        dor_inp = dor_inp.strip()
        dor_list = dor_inp.split(',')

        for i in dor_list:
            M_DROPOUT_RATE.append(float(i))

        print(M_DROPOUT_RATE)

        f += input_check(M_DEPTH, M_DROPOUT_RATE)

        # Convolutional filter depths at each model depth
        # Either specify only 1 value common across whole model
        # OR
        # Specify exactly M_DEPTH vales, these will be reflected to for decoder
        # M_CHANNELS = [16]
        M_CHANNELS = list()
        chan_inp = self.dlg.Build_dout_2.value()
        chan_inp = chan_inp.strip()
        chan_list = chan_inp.split(',')

        for i in chan_list:
            M_CHANNELS.append(int(i))
        print(M_CHANNELS)

        f += input_check(M_DEPTH, M_CHANNELS)

        # Kernel/Filter dimensions
        # Either specify only 1 value common across whole model
        # OR
        # Specify exactly M_DEPTH vales, these will be reflected to for decoder
        # M_KERNEL_SIZE = [3]
        M_KERNEL_SIZE = list()
        ker_inp = self.dlg.Build_dout_4.value()
        ker_inp = ker_inp.strip()
        ker_list = ker_inp.split(',')

        for i in ker_list:
            M_KERNEL_SIZE.append(int(i))
        print(M_KERNEL_SIZE)

        f += input_check(M_DEPTH, M_KERNEL_SIZE)

        # Number of convolutional layers per CONVBLOCK
        # Either specify only 1 value common across whole model
        # OR
        # Specify exactly M_DEPTH vales, these will be reflected to for decoder
        # M_CONV_PER_CONVBLOCK = [1]
        M_CONV_PER_CONVBLOCK = list()
        conv_inp = self.dlg.Build_dout_5.value()
        conv_inp = conv_inp.strip()
        conv_list = conv_inp.split(',')

        for i in conv_list:
            M_CONV_PER_CONVBLOCK.append(int(i))
        print(M_CONV_PER_CONVBLOCK)

        f += input_check(M_DEPTH, M_CONV_PER_CONVBLOCK)

        # Number of convolutional layers in ResBlock within CONVBLOCK
        # M_RES_PER_CONVBLOCK = [1]
        M_RES_PER_CONVBLOCK = list()
        res_inp = self.dlg.Build_dout_3.value()
        res_inp = res_inp.strip()
        res_list = res_inp.split(',')

        for i in res_list:
            M_RES_PER_CONVBLOCK.append(int(i))
        print(M_RES_PER_CONVBLOCK)

        f += input_check(M_DEPTH, M_RES_PER_CONVBLOCK)

        print(f)

        if (f != 0):
            QMessageBox.critical(self.dlg, 'Invalid Input',
                                 'The number of inputs (for some parameter values) do not match the depth of the model.')
            return

        # Optimizer type
        # options :
        # 'adam'
        # 'adagrad'
        # 'rms'
        # 'sgd'
        # M_OPTIMIZER = 'adam'

        M_OPTIMIZER = self.dlg.Build_optmzr.currentText()

        print(M_OPTIMIZER)
        print(type(M_OPTIMIZER))

        # Learning rate
        # M_LEARNING_RATE = 0.001
        M_LEARNING_RATE = self.dlg.Model_Learning_rate.value()

        # Enable Batch norm
        if self.dlg.checkBox_4.isChecked():
            M_BATCH_NORM = True
        else:
            M_BATCH_NORM = False

        # Select activation function
        # options :
        # 'relu'
        # 'sigmoid'
        # 'tanh'
        # M_ACTIVATION = 'relu'

        M_ACTIVATION = self.dlg.Build_ActFunc.currentText()

        # Conditional Statements----------------------------------------------------

        # Optimizer specific variables
        if (M_OPTIMIZER == 'adam'):
            BETA_1 = 0.9
            BETA_2 = 0.999
            EPSILON = 1e-07
            OPTIMIZER = tf.keras.optimizers.Adam(M_LEARNING_RATE, BETA_1, BETA_2, EPSILON)

        elif (M_OPTIMIZER == 'adagrad'):
            IAV = 0.1
            EPSILON = 1e-07
            OPTIMIZER = tf.keras.optimizers.Adagrad(M_LEARNING_RATE, IAV, EPSILON)

        elif (M_OPTIMIZER == 'rms'):
            RHO = 0.9
            MOMENTUM = 0.0
            EPSILON = 1e-07
            OPTIMIZER = tf.keras.optimizers.RMSprop(M_LEARNING_RATE, RHO, MOMENTUM, EPSILON)

        elif (M_OPTIMIZER == 'sgd'):
            MOMENTUM = 0.0
            OPTIMIZER = tf.keras.optimizers.SGD(M_LEARNING_RATE, MOMENTUM)

        else:
            raise Exception(
                'Unknown or Unsupported optimizer function was selected. Options are : adam, adagrad, rms, sgd')

        # Process M_DROPOUT_RATE
        if len(M_DROPOUT_RATE) == 1:
            M_DROPOUT_RATE = [M_DROPOUT_RATE[0] for i in range(2 * M_DEPTH - 1)]
        elif len(M_DROPOUT_RATE) == M_DEPTH:
            temp = M_DROPOUT_RATE[:-1]
            temp.reverse()
            M_DROPOUT_RATE = M_DROPOUT_RATE + temp
        else:
            raise Exception(
                'Expected M_DROPOUT_RATE length to be 1 or {}, instead got {}'.format(M_DEPTH, len(M_DROPOUT_RATE)))

        # Process M_CHANNELS
        if len(M_CHANNELS) == 1:
            n = M_CHANNELS[0]
            M_CHANNELS = [n * (2 ** i) for i in range(M_DEPTH)]
            temp = M_CHANNELS[:-1]
            temp.reverse()
            M_CHANNELS = M_CHANNELS + temp
        elif len(M_CHANNELS) == M_DEPTH:
            temp = M_CHANNELS[:-1]
            temp.reverse()
            M_CHANNELS = M_CHANNELS + temp
        else:
            raise Exception('Expected M_CHANNELS length to be 1 or {}, instead got {}'.format(M_DEPTH, len(M_CHANNELS)))

        # Process M_KERNEL_SIZE
        if len(M_KERNEL_SIZE) == 1:
            M_KERNEL_SIZE = [M_KERNEL_SIZE[0] for i in range(2 * M_DEPTH - 1)]
        elif len(M_KERNEL_SIZE) == M_DEPTH:
            temp = M_KERNEL_SIZE[:-1]
            temp.reverse()
            M_KERNEL_SIZE = M_KERNEL_SIZE + temp
        else:
            raise Exception(
                'Expected M_KERNEL_SIZE length to be 1 or {}, instead got {}'.format(M_DEPTH, len(M_KERNEL_SIZE)))

        # Process M_CONV_PER_CONVBLOCK
        if len(M_CONV_PER_CONVBLOCK) == 1:
            M_CONV_PER_CONVBLOCK = [M_CONV_PER_CONVBLOCK[0] for i in range(2 * M_DEPTH - 1)]
        elif len(M_CONV_PER_CONVBLOCK) == M_DEPTH:
            temp = M_CONV_PER_CONVBLOCK[:-1]
            temp.reverse()
            M_CONV_PER_CONVBLOCK = M_CONV_PER_CONVBLOCK + temp
        else:
            raise Exception('Expected M_CONV_PER_CONVBLOCK length to be 1 or {}, instead got {}'.format(M_DEPTH, len(
                M_CONV_PER_CONVBLOCK)))

        # Process M_RES_PER_CONVBLOCK
        if len(M_RES_PER_CONVBLOCK) == 1:
            M_RES_PER_CONVBLOCK = [M_RES_PER_CONVBLOCK[0] for i in range(2 * M_DEPTH - 1)]
            M_RES_PER_CONVBLOCK[M_DEPTH - 1] = 0
        elif len(M_RES_PER_CONVBLOCK) == M_DEPTH - 1:
            temp = M_RES_PER_CONVBLOCK
            temp.reverse()
            M_RES_PER_CONVBLOCK = M_RES_PER_CONVBLOCK + [0] + temp
        else:
            raise Exception('Expected M_RES_PER_CONVBLOCK length to be 1 or {}, instead got {}'.format(M_DEPTH, len(
                M_RES_PER_CONVBLOCK)))

        # Model Architecture------------------------------------------------------

        def Conv_Block_E(num_filters, kernel_size, layer_num, num_convs, num_res, input_tensor):
            '''
            Convolutional block - Encoder
            in -> Conv layers -> Res layers -> out
            '''
            x = input_tensor

            for i in range(1, num_convs + 1):
                x = Conv2D(num_filters, kernel_size, 1, padding='same', name='conv_{}_{}'.format(layer_num, i))(x)
                if M_BATCH_NORM:
                    x = BatchNormalization(name='batch_norm_{}_{}'.format(layer_num, i))(x)
                x = Activation(M_ACTIVATION, name='activation_{}_{}'.format(layer_num, i))(x)

            if (num_res > 0):
                y = x

                for i in range(1, num_res + 1):
                    y = Conv2D(num_filters, kernel_size, 1, padding='same', name='res_{}_{}'.format(layer_num, i))(y)
                    if M_BATCH_NORM:
                        y = BatchNormalization(name='res_batch_norm_{}_{}'.format(layer_num, i))(y)
                    y = Activation(M_ACTIVATION, name='res_activation_{}_{}'.format(layer_num, i))(y)

                x = Add(name='add_{}'.format(layer_num))([y, x])

            return x

        def Conv_Block_D(num_filters, kernel_size, layer_num, num_convs, num_res, input_tensor):
            '''
            Convolutional block - Decoder
            in -> Res layers -> Conv layers -> out
            '''
            x = input_tensor

            if (num_res > 0):
                y = x

                for i in range(1, num_res + 1):
                    y = Conv2D(num_filters[0], kernel_size, 1, padding='same', name='conv_{}_{}'.format(layer_num, i))(
                        y)
                    if M_BATCH_NORM:
                        y = BatchNormalization(name='batch_norm_{}_{}'.format(layer_num, i))(y)
                    y = Activation(M_ACTIVATION, name='activation_{}_{}'.format(layer_num, i))(y)

                x = Add(name='add_{}'.format(layer_num))([y, x])

            for i in range(1, num_convs + 1):
                x = Conv2D(num_filters[1], kernel_size, 1, padding='same', name='res_{}_{}'.format(layer_num, i))(x)
                if M_BATCH_NORM:
                    x = BatchNormalization(name='res_batch_norm_{}_{}'.format(layer_num, i))(x)
                x = Activation(M_ACTIVATION, name='res_activation_{}_{}'.format(layer_num, i))(x)

            return x

        def Encoder(input_tensor, layer_num):
            '''
            Encoder Block
            in -> Conv_Block_E -> Maxpool -> Dropout -> out
            '''
            conv = Conv_Block_E(M_CHANNELS[layer_num], M_KERNEL_SIZE[layer_num], layer_num,
                                M_CONV_PER_CONVBLOCK[layer_num], M_RES_PER_CONVBLOCK[layer_num], input_tensor)
            pool = MaxPooling2D((2, 2), name='pool_{}'.format(layer_num))(conv)
            drop = Dropout(M_DROPOUT_RATE[layer_num], name='drop_{}'.format(layer_num))(pool)
            return drop, conv

        def Decoder(input_tensor, skip_tensor, layer_num):
            '''
            Decoder Block
            in -> Upscale -> Concat with skip_tensor -> Dropout -> Conv_Block_D -> out
            '''
            upsp = Conv2DTranspose(M_CHANNELS[layer_num], 3, strides=(2, 2), padding='same',
                                   name='upsp_{}'.format(layer_num))(input_tensor)
            cnct = concatenate([upsp, skip_tensor], name='cnct_{}'.format(layer_num))
            drop = Dropout(M_DROPOUT_RATE[layer_num], name='drop_{}'.format(layer_num))(cnct)
            conv = Conv_Block_D([M_CHANNELS[layer_num - 1], M_CHANNELS[layer_num]], M_KERNEL_SIZE[layer_num], layer_num,
                                M_CONV_PER_CONVBLOCK[layer_num], M_RES_PER_CONVBLOCK[layer_num], drop)
            return conv

        def get_UNET():
            '''
            Returns UNET model specfied according to the variables
            '''
            inputs = Input((None, None, N_CHANNELS))

            skip_connections = list()
            for i in range(M_DEPTH - 1):
                if i == 0:
                    x, conv = Encoder(inputs, i)
                else:
                    x, conv = Encoder(x, i)
                skip_connections.append(conv);

            x = Conv_Block_E(M_CHANNELS[M_DEPTH - 1], M_KERNEL_SIZE[M_DEPTH - 1], M_DEPTH - 1,
                             M_CONV_PER_CONVBLOCK[M_DEPTH - 1], M_RES_PER_CONVBLOCK[M_DEPTH - 1], x)

            skip_connections.reverse()
            for i in range(M_DEPTH, 2 * M_DEPTH - 1):
                x = Decoder(x, skip_connections[i - M_DEPTH], i)

            classify = Conv2D(N_CLASSES, 1, 1, activation='sigmoid')(x)

            model = tf.keras.models.Model(inputs=inputs, outputs=classify)

            return model

        def compile_model(model):
            model.compile(optimizer=OPTIMIZER,
                          loss='binary_crossentropy',
                          metrics=[
                              # jaccard_coef,
                              # jaccard_coef_int,
                              'accuracy']
                          )
            return model

        model = get_UNET()
        compile_model(model)

        # Save Model--------------------------------------------------------------------
        model_architecture = model.to_json()

        json_name = 'Dynamic_UNET_test.json'

        out_path = os.path.join(OUT_ADD, json_name)

        with open(out_path, 'w') as json_file:
            json_file.write(model_architecture)

        QMessageBox.information(self.dlg, 'Process Completed', 'The model is successfully saved.')

    def UNET_train(self):

        # INPUT AND PARAMETERS---------------------------------------------

        IMG_ADD = self.dlg.Train_img_add.filePath()
        MASK_ADD = self.dlg.Train_img_label.filePath()
        MODEL_ADD = self.dlg.Train_model.filePath()
        OUT_ADD = self.dlg.Train_Out.filePath()

        if (not IMG_ADD):
            QMessageBox.critical(self.dlg, 'Invalid Input', 'Enter the address of Image to be classified.')
            return

        if (not MODEL_ADD):
            QMessageBox.critical(self.dlg, 'Invalid Input', 'Enter the address of saved model.')
            return

        if (not MASK_ADD):
            QMessageBox.critical(self.dlg, 'Invalid Input', 'Enter the address of the image mask/label.')
            return

        # Resize parameters
        IMG_WIDTH = 512
        IMG_HEIGHT = 512
        IMG_SIZE = (512, 512, 3)

        BATCH_SIZE = self.dlg.Train_bsize.value()
        NUM_EPOCHS = self.dlg.Train_epoch.value()
        VAL_SPLIT = 0.2

        if (not BATCH_SIZE):
            BATCH_SIZE = int(self.dlg.Train_bsize.defaultValue())  # default = 16
        else:
            BATCH_SIZE = int(BATCH_SIZE)

        if (not NUM_EPOCHS):
            NUM_EPOCHS = int(self.dlg.Train_epoch.defaultValue())  # default = 30
        else:
            NUM_EPOCHS = int(NUM_EPOCHS)

        # IMPORT STATEMENTS---------------------------------------------

        try:
            import tensorflow as tf
            import tensorflow.keras.backend as K
            from tensorflow.keras.layers import (Dropout,
                                                 Conv2D,
                                                 MaxPooling2D,
                                                 Conv2DTranspose,
                                                 concatenate,
                                                 BatchNormalization,
                                                 Activation,
                                                 Conv2DTranspose,
                                                 Add)
            from tensorflow.keras import Input
            from tensorflow.keras.callbacks import (EarlyStopping,
                                                    ModelCheckpoint,
                                                    ReduceLROnPlateau)
            from tensorflow.keras.preprocessing.image import (ImageDataGenerator,
                                                              array_to_img,
                                                              img_to_array,
                                                              load_img)

        except ImportError:
            print("tensorflow not present\nInstalling...")
            # import pip
            # pip.main(["install", "--user", "tensorflow"])

            subprocess.call("pip install --user tensorflow", creationflags=subprocess.CREATE_NEW_CONSOLE)

            import tensorflow as tf
            import tensorflow.keras.backend as K
            from tensorflow.keras.layers import (Dropout,
                                                 Conv2D,
                                                 MaxPooling2D,
                                                 Conv2DTranspose,
                                                 concatenate,
                                                 BatchNormalization,
                                                 Activation,
                                                 Conv2DTranspose,
                                                 Add)
            from tensorflow.keras import Input
            from tensorflow.keras.callbacks import (EarlyStopping,
                                                    ModelCheckpoint,
                                                    ReduceLROnPlateau)
            from tensorflow.keras.preprocessing.image import (ImageDataGenerator,
                                                              array_to_img,
                                                              img_to_array,
                                                              load_img)

        try:
            from skimage.transform import resize

        except ImportError:
            print("scikit-image package not present\nInstalling...")
            # import pip
            # pip.main(["install", "--user", "scikit-image"])

            subprocess.call("pip install --user scikit-image", creationflags=subprocess.CREATE_NEW_CONSOLE)

            from skimage.transform import resize

        try:
            from sklearn.model_selection import train_test_split

        except ImportError:
            print("scikit-learn package not present\nInstalling...")
            # import pip
            # pip.main(["install", "--user", "scikit-learn"])

            subprocess.call("pip install --user scikit-learn", creationflags=subprocess.CREATE_NEW_CONSOLE)

            from sklearn.model_selection import train_test_split

        img_ids = os.listdir(IMG_ADD)

        X = np.zeros((len(img_ids), IMG_HEIGHT, IMG_WIDTH, 3), dtype=np.float32)
        y = np.zeros((len(img_ids), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.float32)

        print("Loading Data...")
        for idx, img_id in enumerate(img_ids):
            # Load Images

            img_path = os.path.join(IMG_ADD, img_id)
            mask_path = os.path.join(MASK_ADD, img_id[:-1])

            x_img = img_to_array(load_img(img_path))
            x_img = resize(x_img, (512, 512), mode='constant', preserve_range=True)
            # Load masks
            mask = img_to_array(load_img(mask_path, color_mode="grayscale"))
            mask = resize(mask, (512, 512, 1), mode='constant', preserve_range=True)
            # Save images
            X[idx] = x_img
            y[idx] = (mask / 255) > 0
            if idx % 10 == 0:
                print('#', end='')

        # Train-Test Split
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=VAL_SPLIT, random_state=0)

        print("\nData Loaded")

        # Free up ram
        del X
        del y
        del img_ids

        with open(MODEL_ADD, 'r') as json_file:
            loaded_nnet = json_file.read()

        save_model = tf.keras.models.model_from_json(loaded_nnet)

        save_model.compile(optimizer='adam',
                           loss='binary_crossentropy',
                           metrics=[
                               # jaccard_coef,
                               # jaccard_coef_int,
                               'accuracy']
                           )

        save_model.fit(X_train,
                       y_train,
                       validation_data=(X_val, y_val),
                       validation_freq=5,
                       batch_size=BATCH_SIZE,
                       epochs=NUM_EPOCHS,
                       )

        # Classification Report
        class_predict = save_model.predict(X_test)
        class_report = classification_report(y_test, class_predict, output_dict=True)
        df = pd.DataFrame(class_report).transpose()
        report_file = "Classification_Report_UNET.csv"
        report_path = os.path.join(OUT_ADD, report_file)
        
        try:
            df.to_csv(report_path)
        except PermissionError:
            QMessageBox.critical(self.dlg, 'Permission Denied',
                                 'Unable to save the file in the specified location. Please choose a different address to save the file.')
            return

        weight_file = 'UNET_Model_Weights.h5'
        weight_path = os.path.join(OUT_ADD, weight_file)

        save_model.save_weights(weight_path)

        QMessageBox.information(self.dlg, 'Process Completed', 'The model is successfully trained.')

    # -------------------------------------------WORKFLOW---------------------------------------------------------
    def parameterenabling(self):  # In Train NN Combo box
        i = self.dlg.Method_comboBox.currentIndex()
        print(i)
        if i == 0:
            self.dlg.Train_bsize.setEnabled(True)
            self.dlg.Train_epoch.setEnabled(True)
            # self.dlg.HiddenLayer_comboBox.setEnabled(True)
        if i == 1:
            self.dlg.Train_bsize.setEnabled(True)
            self.dlg.Train_epoch.setEnabled(True)
            # self.dlg.HiddenLayer_comboBox.setEnabled(True)
        if i == 2:
            self.dlg.Train_bsize.setEnabled(True)
            self.dlg.Train_epoch.setEnabled(True)
            # self.dlg.HiddenLayer_comboBox.setEnabled(True)

    def parameterenabling1(self):
        i = self.dlg.Method_comboBox_3.currentIndex()
        print(i)
        if i == 0:
            self.dlg.train_RFC_trees.setEnabled(True)
            self.dlg.train_RFC_Depth.setEnabled(True)

        if i == 1:
            self.dlg.train_RFC_trees.setEnabled(False)
            self.dlg.train_RFC_Depth.setEnabled(False)

        return i

    def parameterenabling2(self):
        i = self.dlg.Classifier_comboBox.currentIndex()
        print(i)
        if i == 0:
            self.dlg.input_img_box_6.setEnabled(True)
            self.dlg.input_img_box.setStorageMode(1)  # To take folder as input

        if i == 1:
            self.dlg.input_img_box_6.setEnabled(True)
            self.dlg.input_img_box.setStorageMode(1)  # To take folder as input

        if i == 2:
            self.dlg.input_img_box_6.setEnabled(False)
            self.dlg.input_img_box.setStorageMode(0)  # To take file as input

        return i

    def Run_Train(self):
        i = self.dlg.Method_comboBox_3.currentIndex()
        print(i)
        if i == 0:
            self.rfc_train()

        if i == 1:
            # self.print_check()
            self.svm_train()

    def Run_Classifier(self):
        i = self.dlg.Classifier_comboBox.currentIndex()
        print(i)
        if i == 0:
            self.UNET_Classifier()

        if i == 1:
            self.SatNet_Classifier()

        if i == 2:
            # self.print_check()
            self.randomForest()

    # ---------------------------------------------------------------------------------------------------------------

    def print_check(self):
        print("check")

    def merge_tiles(self):
        input_path = self.dlg.input_img_box.filePath()
        # output_path = self.dlg.input_img_box_3.filePath()
        output_path = 'C:/Users/HP/Desktop/Tile'  # Output location needs to be looked at
        tiles = list()
        for tile in glob.glob(input_path + "/" + "*.tif"):
            tiles.append(tile)

        processing.run("gdal:merge", {'INPUT': tiles, 'PCT': 'False',
                                      'SEPERATE': 'False', 'DATA_TYPE': 1, 'NODATA_INPUT': None, 'NODATA_OUTPUT': None,
                                      'OPTIONS': 'High Compression', 'EXTRA': 'None',
                                      'OUTPUT': str(output_path) + '/' + 'Merge' + '.tif'})
        print("Tiles Merged!!")

        # ---------------------- help box------------------------------------------#

    def help_head1(self):

        self.dlg.htextBrowser_tab1.setFontUnderline(True)
        self.dlg.htextBrowser_tab1.setFontPointSize(15)
        self.dlg.htextBrowser_tab1.setFontWeight(75)
        self.dlg.htextBrowser_tab1.setText('Help:')

    def help_head2(self):

        self.dlg.htextBrowser_tab2.setFontUnderline(True)
        self.dlg.htextBrowser_tab2.setFontPointSize(15)
        self.dlg.htextBrowser_tab2.setFontWeight(75)
        self.dlg.htextBrowser_tab2.setText('Help:')

    def help_head3(self):

        # self.dlg.htextBrowser_tab3.setFontUnderline(True)
        self.dlg.htextBrowser_tab3.setFontPointSize(12)
        self.dlg.htextBrowser_tab3.setFontWeight(70)
        self.dlg.htextBrowser_tab3.setText('Help Box\n')

    def help_head4(self):

        self.dlg.htextBrowser_tab4.setFontUnderline(True)
        self.dlg.htextBrowser_tab4.setFontPointSize(15)
        self.dlg.htextBrowser_tab4.setFontWeight(75)
        self.dlg.htextBrowser_tab4.setText('Help:')

    def help_head5(self):

        self.dlg.htextBrowser_tab5.setFontUnderline(True)
        self.dlg.htextBrowser_tab5.setFontPointSize(15)
        self.dlg.htextBrowser_tab5.setFontWeight(75)
        self.dlg.htextBrowser_tab5.setText('Help:')

    def InpF1_help(self, event):

        self.help_head1()
        self.dlg.htextBrowser_tab1.setFontUnderline(False)
        self.dlg.htextBrowser_tab1.setFontPointSize(10)
        self.dlg.htextBrowser_tab1.setFontWeight(50)
        self.dlg.htextBrowser_tab1.append(
            'This input field is to specify the location of the image to be segregated into tiles.\nIt should be a raster image i.e in .tiff format')

    def InpF2_help(self, event):

        self.help_head1()
        self.dlg.htextBrowser_tab1.setFontUnderline(False)
        self.dlg.htextBrowser_tab1.setFontPointSize(10)
        self.dlg.htextBrowser_tab1.setFontWeight(50)
        self.dlg.htextBrowser_tab1.append(
            'This input field is to specify the location of the label to be segregated into tiles.\nIt should be a vector or a raster(i.e .tiff format)')

    def InpF3_help(self, event):

        self.help_head1()
        self.dlg.htextBrowser_tab1.setFontUnderline(False)
        self.dlg.htextBrowser_tab1.setFontPointSize(10)
        self.dlg.htextBrowser_tab1.setFontWeight(50)
        self.dlg.htextBrowser_tab1.append(
            'This input field is to specify the output location where the generated tiles of image and label are to be saved')

    def textF1_help(self, event):

        self.help_head1()
        self.dlg.htextBrowser_tab1.setFontUnderline(False)
        self.dlg.htextBrowser_tab1.setFontPointSize(10)
        self.dlg.htextBrowser_tab1.setFontWeight(50)
        self.dlg.htextBrowser_tab1.append(
            'This is to set the number of tiles the image will be divided horizontally.\nIts default value is 500')

    def textF2_help(self, event):

        self.help_head1()
        self.dlg.htextBrowser_tab1.setFontUnderline(False)
        self.dlg.htextBrowser_tab1.setFontPointSize(10)
        self.dlg.htextBrowser_tab1.setFontWeight(50)
        self.dlg.htextBrowser_tab1.append(
            'This is to set the number of tiles the image will be divided vertically.\nIts default value is 700')

    def InpF4_help(self, event):

        self.help_head3()
        self.dlg.htextBrowser_tab3.setFontUnderline(False)
        self.dlg.htextBrowser_tab3.setFontPointSize(10)
        self.dlg.htextBrowser_tab3.setFontWeight(50)
        self.dlg.htextBrowser_tab3.append(
            'This input field is to specify the location of the folder containing tiles of the image.\nThe tiles should be in raster format(.tiff)')

    def InpF5_help(self, event):

        self.help_head3()
        self.dlg.htextBrowser_tab3.setFontUnderline(False)
        self.dlg.htextBrowser_tab3.setFontPointSize(10)
        self.dlg.htextBrowser_tab3.setFontWeight(50)
        self.dlg.htextBrowser_tab3.append(
            'This input field is to specify the location of the folder containing tiles of the mask (label) image.\nThe tiles should be in raster format(.tiff)')

    def InpF6_help(self, event):

        self.help_head3()
        self.dlg.htextBrowser_tab3.setFontUnderline(False)
        self.dlg.htextBrowser_tab3.setFontPointSize(10)
        self.dlg.htextBrowser_tab3.setFontWeight(50)
        self.dlg.htextBrowser_tab3.append(
            'This input field is to specify the location of the saved model.\nThe model should be in H5 file format.')

    def InpF7_help(self, event):

        self.help_head3()
        self.dlg.htextBrowser_tab3.setFontUnderline(False)
        self.dlg.htextBrowser_tab3.setFontPointSize(10)
        self.dlg.htextBrowser_tab3.setFontWeight(50)
        self.dlg.htextBrowser_tab3.append(
            'This input field is to specify the output location where the trained data file is to be saved.\nThe file will be saved as a JSON file.')

    def textF12_help(self, event):

        self.help_head3()
        self.dlg.htextBrowser_tab3.setFontUnderline(False)
        self.dlg.htextBrowser_tab3.setFontPointSize(9)
        self.dlg.htextBrowser_tab3.setFontWeight(50)
        # self.dlg.htextBrowser_tab3.append(
        #     'To set the Batch Size. It defines the number of training samples that will be propagated through the network (less than or equal to the total number of training samples)\nIts default value is 16')
        self.dlg.htextBrowser_tab3.append(
            'Batch Size: \nNumber of training samples that will be propagated through the network (less than or equal to the total number of training samples)\n\n' +
            'Range: 1 to Size(Training Samples)\n' +
            'Default Value: 16\n\n' +
            'Format: Positive number'
            )


    def textF13_help(self, event):

        self.help_head3()
        self.dlg.htextBrowser_tab3.setFontUnderline(False)
        self.dlg.htextBrowser_tab3.setFontPointSize(10)
        self.dlg.htextBrowser_tab3.setFontWeight(50)
        self.dlg.htextBrowser_tab3.append(
            'To set the Maximum Number of Iteration. It defines the number of times the batch data passes through the algorithm.\nIts default value is 30')

    def InpF8_help(self, event):

        self.help_head4()
        self.dlg.htextBrowser_tab4.setFontUnderline(False)
        self.dlg.htextBrowser_tab4.setFontPointSize(10)
        self.dlg.htextBrowser_tab4.setFontWeight(50)
        self.dlg.htextBrowser_tab4.append(
            'This input field is to specify the location of the image for training of the data.\nIt should be a raster image(i.e in .tiff format)')

    def InpF9_help(self, event):

        self.help_head4()
        self.dlg.htextBrowser_tab4.setFontUnderline(False)
        self.dlg.htextBrowser_tab4.setFontPointSize(10)
        self.dlg.htextBrowser_tab4.setFontWeight(50)
        self.dlg.htextBrowser_tab4.append(
            'This input field is to specify the location of the label file.\nIt should be a vector.')

    def InpF10_help(self, event):

        self.help_head4()
        self.dlg.htextBrowser_tab4.setFontUnderline(False)
        self.dlg.htextBrowser_tab4.setFontPointSize(10)
        self.dlg.htextBrowser_tab4.setFontWeight(50)
        self.dlg.htextBrowser_tab4.append(
            'This input field is to specify the output location where the trained data file is to be saved.\nThe file will be saved as a pickle file.')

    def textF14_help(self, event):

        self.help_head4()
        self.dlg.htextBrowser_tab4.setFontUnderline(False)
        self.dlg.htextBrowser_tab4.setFontPointSize(10)
        self.dlg.htextBrowser_tab4.setFontWeight(50)
        self.dlg.htextBrowser_tab4.append(
            'To set the number of trees. This parameters decides the total number of trees in your random forest classifier.\nIts default value is set to 20.')

    def textF15_help(self, event):

        self.help_head4()
        self.dlg.htextBrowser_tab4.setFontUnderline(False)
        self.dlg.htextBrowser_tab4.setFontPointSize(10)
        self.dlg.htextBrowser_tab4.setFontWeight(50)
        self.dlg.htextBrowser_tab4.append(
            'To set the Depth.This parameters decides how deep the trees in the random forest will be. Deeper trees often lead to overfitting and also increase the training time.\nBy default it is set to None (Maximum Depth)')

    def InpF11_help(self, event):

        self.help_head5()
        self.dlg.htextBrowser_tab5.setFontUnderline(False)
        self.dlg.htextBrowser_tab5.setFontPointSize(10)
        self.dlg.htextBrowser_tab5.setFontWeight(50)
        self.dlg.htextBrowser_tab5.append(
            'This input field is to specify the location of image in case of Non-Neural network based classifier and location of folder containing tiles of image in case of Neural network based classifier.\nThe image or tiles should be in raster format(i.e .tiff)')

    def InpF12_help(self, event):

        self.help_head5()
        self.dlg.htextBrowser_tab5.setFontUnderline(False)
        self.dlg.htextBrowser_tab5.setFontPointSize(10)
        self.dlg.htextBrowser_tab5.setFontWeight(50)
        self.dlg.htextBrowser_tab5.append(
            'This input field is to specify the location of trained model file.\nThe model file should be a JSON file in case of Neural Network based classification & a Pickle file in case of Non-Neural Network based classification')

    def InpF13_help(self, event):

        self.help_head5()
        self.dlg.htextBrowser_tab5.setFontUnderline(False)
        self.dlg.htextBrowser_tab5.setFontPointSize(10)
        self.dlg.htextBrowser_tab5.setFontWeight(50)
        self.dlg.htextBrowser_tab5.append(
            'This input is required only for Neural Network based classification.\nThis input field is to specify the location of model weights.\nThe model weights should be in H5 file format')

    def InpF14_help(self, event):

        self.help_head5()
        self.dlg.htextBrowser_tab5.setFontUnderline(False)
        self.dlg.htextBrowser_tab5.setFontPointSize(10)
        self.dlg.htextBrowser_tab5.setFontWeight(50)
        self.dlg.htextBrowser_tab5.append(
            'This input field is to specify the output location where the classified image is to be saved.\nThe image will be saved as a raster image(i.e .tiff format)')

    def InpF15_help(self, event):

        self.help_head2()
        self.dlg.htextBrowser_tab2.setFontUnderline(False)
        self.dlg.htextBrowser_tab2.setFontPointSize(10)
        self.dlg.htextBrowser_tab2.setFontWeight(50)
        self.dlg.htextBrowser_tab2.append(
            'This input field is to specify the output location where the built model is to be saved.\nThe model will be saved in H5 file format.')

    def textF3_help(self, event):

        self.help_head2()
        self.dlg.htextBrowser_tab2.setFontUnderline(False)
        self.dlg.htextBrowser_tab2.setFontPointSize(10)
        self.dlg.htextBrowser_tab2.setFontWeight(50)
        self.dlg.htextBrowser_tab2.append(
            'To set the number of bands. Set this parameter equal to the number of bands in your input image.\nIts default value is 3')

    def textF4_help(self, event):

        self.help_head2()
        self.dlg.htextBrowser_tab2.setFontUnderline(False)
        self.dlg.htextBrowser_tab2.setFontPointSize(10)
        self.dlg.htextBrowser_tab2.setFontWeight(50)
        self.dlg.htextBrowser_tab2.append(
            'To set the number classes (e.g. Forest, water bodies, build up land etc) you wish to classify.\nIts default value is 1')

    def textF5_help(self, event):

        self.help_head2()
        self.dlg.htextBrowser_tab2.setFontUnderline(False)
        self.dlg.htextBrowser_tab2.setFontPointSize(10)
        self.dlg.htextBrowser_tab2.setFontWeight(50)
        self.dlg.htextBrowser_tab2.append(
            'To set the Depth. This parameter will determine the number depth of your convolution layers i.e. the number of kernels you want in the convolution layers.\nIts default value is 5')

    def textF6_help(self, event):

        self.help_head2()
        self.dlg.htextBrowser_tab2.setFontUnderline(False)
        self.dlg.htextBrowser_tab2.setFontPointSize(10)
        self.dlg.htextBrowser_tab2.setFontWeight(50)
        self.dlg.htextBrowser_tab2.append(
            'To set the Kernel size. If you wish to build a CNN based model, then the kernel will act as a filter that is used to extract the features from the image.The kernel is a matrix that moves over the input data, performs the dot product with the sub-region of input data, and gets the output as the matrix of dot products.\nIts default value is 3')

    def textF7_help(self, event):

        self.help_head2()
        self.dlg.htextBrowser_tab2.setFontUnderline(False)
        self.dlg.htextBrowser_tab2.setFontPointSize(10)
        self.dlg.htextBrowser_tab2.setFontWeight(50)
        self.dlg.htextBrowser_tab2.append(
            'To set the Dropout Rate. It is a way of performing regularization. Regularization prevents overfitting and makes the model more robust.\nThe range for this parameter is  (0,1).\nIts default value is 0.2')

    def textF8_help(self, event):

        self.help_head2()
        self.dlg.htextBrowser_tab2.setFontUnderline(False)
        self.dlg.htextBrowser_tab2.setFontPointSize(10)
        self.dlg.htextBrowser_tab2.setFontWeight(50)
        self.dlg.htextBrowser_tab2.append(
            'To set the number of channels (bands). Set this equal to the number of channels (bands) in the input image.\nIts default value is 16')

    def textF9_help(self, event):

        self.help_head2()
        self.dlg.htextBrowser_tab2.setFontUnderline(False)
        self.dlg.htextBrowser_tab2.setFontPointSize(10)
        self.dlg.htextBrowser_tab2.setFontWeight(50)
        self.dlg.htextBrowser_tab2.append(
            'For unet classifier, specify the number of convolutional layers you want your input to pass through before entering the Transpose layer.\nIts default value is 1.\nRefer to model architecture of UNET for more information.')

    def textF10_help(self, event):

        self.help_head2()
        self.dlg.htextBrowser_tab2.setFontUnderline(False)
        self.dlg.htextBrowser_tab2.setFontPointSize(10)
        self.dlg.htextBrowser_tab2.setFontWeight(50)
        self.dlg.htextBrowser_tab2.append(
            'To set the Learning Rate.This determines how quickly the model adapts to the data.\nIt should be a positive number lying between 0.0 and 1.0.\nIts default value is 0.001')

    def textF11_help(self, event):

        self.help_head2()
        self.dlg.htextBrowser_tab2.setFontUnderline(False)
        self.dlg.htextBrowser_tab2.setFontPointSize(10)
        self.dlg.htextBrowser_tab2.setFontWeight(50)
        self.dlg.htextBrowser_tab2.append('This is to input vector\nIts default value is 1')

    def checkbox1_help(self, event):

        self.help_head2()
        self.dlg.htextBrowser_tab2.setFontUnderline(False)
        self.dlg.htextBrowser_tab2.setFontPointSize(10)
        self.dlg.htextBrowser_tab2.setFontWeight(50)
        self.dlg.htextBrowser_tab2.append(
            'To enable Batch Normalization. It is another technique for performing regularization.\nBy default it will be enabled')

    def list1_help(self, event):

        self.help_head2()
        self.dlg.htextBrowser_tab2.setFontUnderline(False)
        self.dlg.htextBrowser_tab2.setFontPointSize(10)
        self.dlg.htextBrowser_tab2.setFontWeight(50)
        self.dlg.htextBrowser_tab2.append(
            'To select the optimizer from the given list.These are algorithms or methods that are used to change the attributes of the neural network such as weights and learning rate in order to reduce the losses.\nChoose any one depending upon the problem and dataset.')

    def list2_help(self, event):

        self.help_head2()
        self.dlg.htextBrowser_tab2.setFontUnderline(False)
        self.dlg.htextBrowser_tab2.setFontPointSize(10)
        self.dlg.htextBrowser_tab2.setFontWeight(50)
        self.dlg.htextBrowser_tab2.append(
            'To set the activation function. In artificial neural networks, the activation function of a node defines the output of that node given an input or set of inputs.\nUse Relu for intermediate layers\nUse sigmoid(binary classification) and softmax(multi class classification) for the output (last) layer.')

    def list3_help(self, event):

        self.help_head3()
        self.dlg.htextBrowser_tab3.setFontUnderline(False)
        self.dlg.htextBrowser_tab3.setFontPointSize(10)
        self.dlg.htextBrowser_tab3.setFontWeight(50)
        self.dlg.htextBrowser_tab3.append(
            'To select the classification method for which training of data is to be done')

    def list4_help(self, event):

        self.help_head3()
        self.dlg.htextBrowser_tab3.setFontUnderline(False)
        self.dlg.htextBrowser_tab3.setFontPointSize(10)
        self.dlg.htextBrowser_tab3.setFontWeight(50)
        self.dlg.htextBrowser_tab3.append(
            'To set the train validation ratio.\nIt is the ratio for spliting your data into training and validation')

    def list5_help(self, event):

        self.help_head4()
        self.dlg.htextBrowser_tab4.setFontUnderline(False)
        self.dlg.htextBrowser_tab4.setFontPointSize(10)
        self.dlg.htextBrowser_tab4.setFontWeight(50)
        self.dlg.htextBrowser_tab4.append(
            'To select the classification method for which training of data is to be done')

    def list6_help(self, event):

        self.help_head4()
        self.dlg.htextBrowser_tab4.setFontUnderline(False)
        self.dlg.htextBrowser_tab4.setFontPointSize(10)
        self.dlg.htextBrowser_tab4.setFontWeight(50)
        self.dlg.htextBrowser_tab4.append(
            'To set the train validation ratio.\nIt is the ratio for spliting your data into training and validation')

    def list7_help(self, event):

        self.help_head5()
        self.dlg.htextBrowser_tab5.setFontUnderline(False)
        self.dlg.htextBrowser_tab5.setFontPointSize(10)
        self.dlg.htextBrowser_tab5.setFontWeight(50)
        self.dlg.htextBrowser_tab5.append('To select the classification model for the classification of image')

    def clearbrowser(self, event):

        self.dlg.htextBrowser_tab1.clear()
        self.dlg.htextBrowser_tab2.clear()
        self.dlg.htextBrowser_tab3.clear()
        self.dlg.htextBrowser_tab4.clear()
        self.dlg.htextBrowser_tab5.clear()



    # --------------------------------------------DIALOG BOX------------------------------------------------------

    def run(self):
        """Run method that performs all the real work"""

        # Create the dialog with elements (after translation) and keep reference
        # Only create GUI ONCE in callback, so that it will only load when the plugin is started
        if self.first_start == True:
            self.first_start = False
            self.dlg = RandomForestClassifierDialog()

        # Progress bar
        self.dlg.Tile_progressBar.setValue(0)
        self.dlg.train_progressBar.setValue(0)
        self.dlg.Clfr_progressBar.setValue(0)

        # Temporary
        self.dlg.classifier_output.setEnabled(True)
        self.dlg.train_output.setEnabled(True)
        self.dlg.Output_Field_2.setEnabled(False)
        self.dlg.Tiles_Output.setEnabled(True)

        # show the dialog
        self.dlg.show()

        # ----------------------Initiating text in Text Brower----------------------------------------
        # tab1
        # self.dlg.mtextBrowser_tab1.setFontUnderline(True)
        # self.dlg.mtextBrowser_tab1.setFontPointSize(19)
        # self.dlg.mtextBrowser_tab1.setFontWeight(75)
        # self.dlg.mtextBrowser_tab1.setText('Generate Tiles Tab:')

        # self.dlg.mtextBrowser_tab1.setFontUnderline(False)
        # self.dlg.mtextBrowser_tab1.setFontPointSize(10)
        # self.dlg.mtextBrowser_tab1.setFontWeight(50)
        # self.dlg.mtextBrowser_tab1.append('This Tab is to Generate Tiles of specified size of the image and label')
        # # tab2
        # self.dlg.mtextBrowser_tab2.setFontUnderline(True)
        # self.dlg.mtextBrowser_tab2.setFontPointSize(19)
        # self.dlg.mtextBrowser_tab2.setFontWeight(75)
        # self.dlg.mtextBrowser_tab2.setText('Build Model Tab:')

        # self.dlg.mtextBrowser_tab2.setFontUnderline(False)
        # self.dlg.mtextBrowser_tab2.setFontPointSize(10)
        # self.dlg.mtextBrowser_tab2.setFontWeight(50)
        # self.dlg.mtextBrowser_tab2.append('This Tab is to Build Model')
        # # tab3
        # self.dlg.mtextBrowser_tab3.setFontUnderline(True)
        # self.dlg.mtextBrowser_tab3.setFontPointSize(19)
        # self.dlg.mtextBrowser_tab3.setFontWeight(75)
        # self.dlg.mtextBrowser_tab3.setText('Train Data(NN) Tab:')

        # self.dlg.mtextBrowser_tab3.setFontUnderline(False)
        # self.dlg.mtextBrowser_tab3.setFontPointSize(10)
        # self.dlg.mtextBrowser_tab3.setFontWeight(50)
        # self.dlg.mtextBrowser_tab3.append(
        #     'This Tab is to Train Data for neural network based classifiers by setting concerned hyperparameter')
        # # tab4
        # self.dlg.mtextBrowser_tab4.setFontUnderline(True)
        # self.dlg.mtextBrowser_tab4.setFontPointSize(19)
        # self.dlg.mtextBrowser_tab4.setFontWeight(75)
        # self.dlg.mtextBrowser_tab4.setText('Train Data(other) Tab:')

        # self.dlg.mtextBrowser_tab4.setFontUnderline(False)
        # self.dlg.mtextBrowser_tab4.setFontPointSize(10)
        # self.dlg.mtextBrowser_tab4.setFontWeight(50)
        # self.dlg.mtextBrowser_tab4.append(
        #     'This Tab is to Train Data for non-neural network based classifiers by setting concerned hyperparameter')
        # # tab5
        # self.dlg.mtextBrowser_tab5.setFontUnderline(True)
        # self.dlg.mtextBrowser_tab5.setFontPointSize(19)
        # self.dlg.mtextBrowser_tab5.setFontWeight(75)
        # self.dlg.mtextBrowser_tab5.setText('Classifier Tab:')

        # self.dlg.mtextBrowser_tab5.setFontUnderline(False)
        # self.dlg.mtextBrowser_tab5.setFontPointSize(10)
        # self.dlg.mtextBrowser_tab5.setFontWeight(50)
        # self.dlg.mtextBrowser_tab5.append(
        #     'This Tab is to Classify the image according to the choice of the classifier.')

        # --------------------Tiles Generation TAB----------------------------------------------

        # Stores entries from the input boxes
        # tr_IMG_ADD = self.dlg.ImageInput_Field.filePath()

        # Calls the function to split image after the button is pressed
        self.dlg.Tiles_Button.clicked.connect(self.tiles)

        self.dlg.Tiles_Input.enterEvent = self.InpF1_help
        self.dlg.Tiles_Input.leaveEvent = self.clearbrowser

        self.dlg.Tiles_Input_2.enterEvent = self.InpF2_help
        self.dlg.Tiles_Input_2.leaveEvent = self.clearbrowser

        self.dlg.TileSizeX.enterEvent = self.textF1_help
        self.dlg.TileSizeX.leaveEvent = self.clearbrowser

        self.dlg.TileSizeY.enterEvent = self.textF2_help
        self.dlg.TileSizeY.leaveEvent = self.clearbrowser

        self.dlg.Tiles_Output.enterEvent = self.InpF3_help
        self.dlg.Tiles_Output.leaveEvent = self.clearbrowser


        # ---------------------- BUILD MODEL-------------------------------------------------

        self.dlg.Build_Button.clicked.connect(self.UNET_build)

        self.dlg.Build_bands.enterEvent = self.textF3_help
        self.dlg.Build_bands.leaveEvent = self.clearbrowser

        self.dlg.Build_classes.enterEvent = self.textF4_help
        self.dlg.Build_classes.leaveEvent = self.clearbrowser

        self.dlg.Build_depth.enterEvent = self.textF5_help
        self.dlg.Build_depth.leaveEvent = self.clearbrowser

        self.dlg.Build_dout_4.enterEvent = self.textF6_help
        self.dlg.Build_dout_4.leaveEvent = self.clearbrowser

        self.dlg.Build_dout.enterEvent = self.textF7_help
        self.dlg.Build_dout.leaveEvent = self.clearbrowser

        self.dlg.Build_dout_2.enterEvent = self.textF8_help
        self.dlg.Build_dout_2.leaveEvent = self.clearbrowser

        self.dlg.Build_dout_5.enterEvent = self.textF9_help
        self.dlg.Build_dout_5.leaveEvent = self.clearbrowser

        self.dlg.Model_Learning_rate.enterEvent = self.textF10_help
        self.dlg.Model_Learning_rate.leaveEvent = self.clearbrowser

        self.dlg.Build_dout_3.enterEvent = self.textF11_help
        self.dlg.Build_dout_3.leaveEvent = self.clearbrowser

        self.dlg.Build_out.enterEvent = self.InpF15_help
        self.dlg.Build_out.leaveEvent = self.clearbrowser

        self.dlg.checkBox_4.enterEvent = self.checkbox1_help
        self.dlg.checkBox_4.leaveEvent = self.clearbrowser

        self.dlg.Build_optmzr.enterEvent = self.list1_help
        self.dlg.Build_optmzr.leaveEvent = self.clearbrowser

        self.dlg.Build_ActFunc.enterEvent = self.list2_help
        self.dlg.Build_ActFunc.leaveEvent = self.clearbrowser

        # ---------------------- Train TAB (NN based)-------------------------------------------------

        # for enabeling parameter input widgets
        self.dlg.Method_comboBox.activated.connect(self.parameterenabling)
        self.dlg.Train_Button.clicked.connect(self.UNET_train)

        self.dlg.Train_img_add.enterEvent = self.InpF4_help
        self.dlg.Train_img_add.leaveEvent = self.clearbrowser

        self.dlg.Train_img_label.enterEvent = self.InpF5_help
        self.dlg.Train_img_label.leaveEvent = self.clearbrowser

        self.dlg.Train_model.enterEvent = self.InpF6_help
        self.dlg.Train_model.leaveEvent = self.clearbrowser

        self.dlg.Output_Field_2.enterEvent = self.InpF7_help
        self.dlg.Output_Field_2.leaveEvent = self.clearbrowser

        self.dlg.Train_bsize.enterEvent = self.textF12_help
        self.dlg.Train_bsize.leaveEvent = self.clearbrowser

        self.dlg.Train_epoch.enterEvent = self.textF13_help
        self.dlg.Train_epoch.leaveEvent = self.clearbrowser

        self.dlg.Method_comboBox.enterEvent = self.list3_help
        self.dlg.Method_comboBox.leaveEvent = self.clearbrowser

        self.dlg.Train_valRatio.enterEvent = self.list4_help
        self.dlg.Train_valRatio.leaveEvent = self.clearbrowser

        # ----------------------------Train TAB-------------------------------------------------------

        self.dlg.Method_comboBox_3.activated.connect(self.parameterenabling1)
        self.dlg.train_button.clicked.connect(self.Run_Train)

        self.dlg.train_img_add.enterEvent = self.InpF8_help
        self.dlg.train_img_add.leaveEvent = self.clearbrowser

        self.dlg.train_img_label.enterEvent = self.InpF9_help
        self.dlg.train_img_label.leaveEvent = self.clearbrowser

        self.dlg.train_output.enterEvent = self.InpF10_help
        self.dlg.train_output.leaveEvent = self.clearbrowser

        self.dlg.train_RFC_trees.enterEvent = self.textF14_help
        self.dlg.train_RFC_trees.leaveEvent = self.clearbrowser

        self.dlg.train_RFC_Depth.enterEvent = self.textF15_help
        self.dlg.train_RFC_Depth.leaveEvent = self.clearbrowser

        self.dlg.Method_comboBox_3.enterEvent = self.list5_help
        self.dlg.Method_comboBox_3.leaveEvent = self.clearbrowser

        self.dlg.train_valRatio.enterEvent = self.list6_help
        self.dlg.train_valRatio.leaveEvent = self.clearbrowser

        # ----------------------------CLASSIFIER TAB----------------------------------------

        # self.dlg.testButton.clicked.connect(QMessageBox(self.iface.mainWindow(), 'Reverse Geocoding Error', 'Wrong Format!\nExiting...'))
        # print(IMG_ADD)

        self.dlg.Classifier_comboBox.activated.connect(self.parameterenabling2)
        self.dlg.RunClassifier_Button.clicked.connect(self.Run_Classifier)

        self.dlg.input_img_box.enterEvent = self.InpF11_help
        self.dlg.input_img_box.leaveEvent = self.clearbrowser

        self.dlg.input_img_box_2.enterEvent = self.InpF12_help
        self.dlg.input_img_box_2.leaveEvent = self.clearbrowser

        self.dlg.input_img_box_6.enterEvent = self.InpF13_help
        self.dlg.input_img_box_6.leaveEvent = self.clearbrowser

        self.dlg.classifier_output.enterEvent = self.InpF14_help
        self.dlg.classifier_output.leaveEvent = self.clearbrowser

        self.dlg.Classifier_comboBox.enterEvent = self.list7_help
        self.dlg.Classifier_comboBox.leaveEvent = self.clearbrowser

        # self.dlg.RunClassifier_Button.clicked.connect(self.merge)

        # --------------------------------------------------------------------------------------------

        # Run the dialog event loop
        result = self.dlg.exec_()
        # See if OK was pressed
        if result:
            # Do something useful here - delete the line containing pass and
            # substitute with your code.
            pass
